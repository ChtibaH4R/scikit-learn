{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChtibaH4R/scikit-learn/blob/main/fireDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVpEmRMbToJ8"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2ubolCzTQeyL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import urllib.request\n",
        "import time\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import (Dense, Dropout, LayerNormalization, Rescaling,\n",
        "                                     Conv2D, MaxPooling2D, Flatten, RandomFlip,\n",
        "                                     RandomRotation, RandomZoom)\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dM2GRhhZYDN"
      },
      "source": [
        "# Définition des variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Wqz3hPSfZcem"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "IMAGE_SIZE = 32\n",
        "PATCH_SIZE = 7\n",
        "NUM_LAYERS = 8\n",
        "NUM_HEADS = 16\n",
        "MLP_DIM = 128\n",
        "lr = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "BATCH_SIZE = 64\n",
        "epochs = 2\n",
        "num_classes = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSPflRWSQ5gm"
      },
      "source": [
        "# Définition des classes pour le modèle Vision Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1gdO87voQzKY"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feedforward_dim, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.multiheadselfattention = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(feedforward_dim, activation=\"relu\"),\n",
        "            Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        out1 = self.layernorm1(inputs)\n",
        "        attention_output = self.multiheadselfattention(out1)\n",
        "        attention_output = self.dropout1(attention_output, training=training)\n",
        "        out2 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out2 + ffn_output)\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "        assert self.embed_dim % self.num_heads == 0\n",
        "        self.projection_dim = self.embed_dim // self.num_heads\n",
        "        self.query_dense = Dense(self.embed_dim)\n",
        "        self.key_dense = Dense(self.embed_dim)\n",
        "        self.value_dense = Dense(self.embed_dim)\n",
        "        self.combine_heads = Dense(self.embed_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        attention_output = tf.matmul(query, key, transpose_b=True) / tf.sqrt(float(self.embed_dim))\n",
        "        attention_output = tf.nn.softmax(attention_output, axis=-1)\n",
        "        output = tf.matmul(attention_output, value)\n",
        "        return self.combine_heads(output)\n",
        "\n",
        "class VisionTransformer(Model):\n",
        "    def __init__(self, image_size, patch_size, num_layers, num_classes, d_model, num_heads, mlp_dim, channels=3, dropout=0.1):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.rescale = Rescaling(1./255)\n",
        "        self.patch_proj = Dense(d_model)\n",
        "        self.enc_layers = [\n",
        "            TransformerBlock(d_model, num_heads, mlp_dim, dropout) for _ in range(num_layers)\n",
        "        ]\n",
        "        self.mlp_head = tf.keras.Sequential([\n",
        "            Dense(mlp_dim), Dropout(dropout), Dense(num_classes)\n",
        "        ])\n",
        "\n",
        "    def extract_patches(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        return tf.reshape(patches, [batch_size, -1, self.patch_size ** 2 * 3])\n",
        "\n",
        "    def call(self, x, training):\n",
        "        x = self.rescale(x)\n",
        "        patches = self.extract_patches(x)\n",
        "        x = self.patch_proj(patches)\n",
        "        for layer in self.enc_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return self.mlp_head(x[:, 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1PhpRHVWte4"
      },
      "source": [
        "# Définition d'un modèle pré-entraîné Vision Transformer (ViT) de Google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4FuyASA6Wt44"
      },
      "outputs": [],
      "source": [
        "# Télécharger les poids du modèle ViT\n",
        "def download_vit_weights():\n",
        "    url = 'https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz'\n",
        "    file_path = 'ViT-B_16.npz'\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Downloading ViT weights from {url}...\")\n",
        "        urllib.request.urlretrieve(url, file_path)\n",
        "        print(\"Download complete!\")\n",
        "    else:\n",
        "        print(\"ViT weights already downloaded.\")\n",
        "\n",
        "    return file_path\n",
        "\n",
        "# Charger les poids téléchargés\n",
        "def load_vit_weights(model, weights):\n",
        "    \"\"\"Charge les poids pré-entraînés dans le modèle ViT\"\"\"\n",
        "    layer_names = [layer.name for layer in model.layers]\n",
        "    for key in weights.files:\n",
        "        if key.startswith('embedding') or key.startswith('transformer'):\n",
        "            # Sélectionner les bonnes couches et appliquer les poids\n",
        "            layer_name = key.split('/')[0]\n",
        "            if layer_name in layer_names:\n",
        "                layer = model.get_layer(layer_name)\n",
        "                layer.set_weights(weights[key])\n",
        "    return model\n",
        "\n",
        "# Définir le modèle ViT\n",
        "class VisionTransformerGoogle(tf.keras.Model):\n",
        "    def __init__(self, num_classes, image_size=32, patch_size=16, embed_dim=768, num_heads=12, num_layers=12, dropout=0.1):\n",
        "        super(VisionTransformerGoogle, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = embed_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Définition de la projection des patches\n",
        "        self.rescale = tf.keras.layers.Rescaling(1./255)\n",
        "        self.patch_proj = tf.keras.layers.Dense(embed_dim)\n",
        "\n",
        "        # Transformer layers\n",
        "        self.enc_layers = [\n",
        "            TransformerBlock(embed_dim, num_heads, embed_dim * 4, dropout) for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        # MLP Head\n",
        "        self.mlp_head = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(embed_dim), tf.keras.layers.Dropout(dropout), tf.keras.layers.Dense(num_classes)\n",
        "        ])\n",
        "\n",
        "    def extract_patches(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        return tf.reshape(patches, [batch_size, -1, self.patch_size ** 2 * 3])\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.rescale(x)\n",
        "        patches = self.extract_patches(x)\n",
        "        x = self.patch_proj(patches)\n",
        "        for layer in self.enc_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return self.mlp_head(x[:, 0])\n",
        "\n",
        "# Charger les poids\n",
        "def download_and_load_weights(model):\n",
        "    weights_path = download_vit_weights()\n",
        "    weights = np.load(weights_path, allow_pickle=True)\n",
        "    model = load_vit_weights(model, weights)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4qPbL4gRFRB"
      },
      "source": [
        "# Définition des classes pour le modèle CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F0i8uYt0RGvt"
      },
      "outputs": [],
      "source": [
        "class CNNModel(Model):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            Rescaling(1./255),\n",
        "            Conv2D(32, (3,3), activation='relu', padding='same'),\n",
        "            MaxPooling2D(),\n",
        "            Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "            MaxPooling2D(),\n",
        "            Flatten(),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dense(num_classes)\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyfiPdI0RnSm"
      },
      "source": [
        "# Charger le dataset depuis mon Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYFffuobRmDp",
        "outputId": "8e501063-fa2d-4aad-dbe9-62ccc7281363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 39385 files belonging to 2 classes.\n",
            "Found 8617 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "training_path = \"/content/drive/MyDrive/data/Training/Training\"\n",
        "test_path = \"/content/drive/MyDrive/data/Test\"\n",
        "\n",
        "ds_train = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    training_path,\n",
        "    seed=123,\n",
        "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "ds_test = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_path,\n",
        "    seed=123,\n",
        "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAHSyJWzSeuo"
      },
      "source": [
        "# Augmentation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dTRuNQwHSe7r"
      },
      "outputs": [],
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "    RandomFlip(\"horizontal\"),\n",
        "    RandomRotation(0.1),\n",
        "    RandomZoom(0.1)\n",
        "])\n",
        "\n",
        "ds_train = ds_train.map(lambda x, y: (data_augmentation(x, training=True), y)).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVL_YHGlSzJr"
      },
      "source": [
        "# Initialisation des modèles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOL7qZB6S03r",
        "outputId": "f49849ce-de7c-4466-e1cb-dd867da2f34f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ViT weights from https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz...\n",
            "Download complete!\n"
          ]
        }
      ],
      "source": [
        "# Initialisation du modèle ViT\n",
        "vit_model = VisionTransformer(\n",
        "    image_size=IMAGE_SIZE,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    num_classes=num_classes,\n",
        "    d_model=64,\n",
        "    num_heads=NUM_HEADS,\n",
        "    mlp_dim=MLP_DIM,\n",
        "    channels=3,\n",
        "    dropout=0.1\n",
        ")\n",
        "vit_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.AdamW(learning_rate=lr, weight_decay=WEIGHT_DECAY),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Initialisation du modèle CNN\n",
        "cnn_model = CNNModel(num_classes)\n",
        "cnn_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Initialisation du modèle Google ViT\n",
        "vit_google_model = VisionTransformerGoogle(num_classes=num_classes)\n",
        "vit_google_model = download_and_load_weights(vit_google_model)\n",
        "vit_google_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                         metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crKkYldoTBg4"
      },
      "source": [
        "# Entraînement des modèles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X1gwg5DTCOX",
        "outputId": "40bfdbbe-4dde-46df-834d-ac82a1cad6da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ViT...\n",
            "Epoch 1/2\n",
            "\u001b[1m101/616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33:10\u001b[0m 11s/step - accuracy: 0.5758 - loss: 0.8421"
          ]
        }
      ],
      "source": [
        "histories = {}\n",
        "models_path = \"/content/drive/MyDrive/models\"\n",
        "# Entraînement des modèles et suivi de l'historique\n",
        "for model_name, model in zip(['ViT', 'GoogleViT', 'CNN' ], [vit_model, vit_google_model, cnn_model]):\n",
        "    start = time.time()\n",
        "    print(f\"Training {model_name}...\")\n",
        "    history = model.fit(ds_train, validation_data=ds_test, epochs=epochs)\n",
        "    model.save(f\"{models_path}/{model_name}.keras\")\n",
        "    histories[model_name] = history\n",
        "    end = time.time()\n",
        "    print(f\"{model_name} trained in {end - start} seconds\")\n",
        "    f = open(f\"histories_{model_name}.pkl\",\"wb\")\n",
        "    pickle.dump(histories,f)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training CNN...\n",
        "Epoch 1/5\n",
        "616/616 ━━━━━━━━━━━━━━━━━━━━ 8538s 14s/step - accuracy: 0.9196 - loss: 0.2096 - val_accuracy: 0.5903 - val_loss: 1.5432\n",
        "Epoch 2/5\n",
        "616/616 ━━━━━━━━━━━━━━━━━━━━ 236s 373ms/step - accuracy: 0.9682 - loss: 0.0937 - val_accuracy: 0.5768 - val_loss: 1.3102\n",
        "Epoch 3/5\n",
        "616/616 ━━━━━━━━━━━━━━━━━━━━ 249s 352ms/step - accuracy: 0.9806 - loss: 0.0560 - val_accuracy: 0.5751 - val_loss: 1.6303\n",
        "Epoch 4/5\n",
        "616/616 ━━━━━━━━━━━━━━━━━━━━ 213s 346ms/step - accuracy: 0.9843 - loss: 0.0442 - val_accuracy: 0.5646 - val_loss: 1.8229\n",
        "Epoch 5/5\n",
        "616/616 ━━━━━━━━━━━━━━━━━━━━ 264s 350ms/step - accuracy: 0.9865 - loss: 0.0406 - val_accuracy: 0.5737 - val_loss: 2.1958\n",
        "CNN trained in 9501.716970205307 seconds\n",
        "\n",
        "Training ViT...\n",
        "Epoch 1/5\n",
        "167/616 ━━━━━━━━━━━━━━━━━━━━ 2:23 319ms/step - accuracy: 0.6261 - loss: 0.6861"
      ],
      "metadata": {
        "id": "HacPh31rupuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# On Test data\n",
        "Training Google ViT...\n",
        "108/108 ━━━━━━━━━━━━━━━━━━━━ 2176s 19s/step - accuracy: 0.5029 - loss: 3.2528 - val_accuracy: 0.6123 - val_loss: 0.6729\n",
        "Google ViT trained in 2186.0178921222687 seconds\n",
        "\n",
        "Training CNN...\n",
        "108/108 ━━━━━━━━━━━━━━━━━━━━ 40s 337ms/step - accuracy: 0.6094 - loss: 0.6609 - val_accuracy: 0.8299 - val_loss: 0.4377\n",
        "CNN trained in 42.477810859680176 seconds\n",
        "\n",
        "Training ViT...\n",
        "108/108 ━━━━━━━━━━━━━━━━━━━━ 98s 435ms/step - accuracy: 0.5381 - loss: 0.7901 - val_accuracy: 0.6135 - val_loss: 0.6815\n",
        "ViT trained in 98.84237217903137 seconds"
      ],
      "metadata": {
        "id": "3BUtzXq5WUgG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUS_UEarVVr1"
      },
      "source": [
        "# Affichage des courbes de performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS501z__VXNc"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "for model_name, history in histories.items():\n",
        "    plt.plot(history.history['accuracy'], label=f'{model_name} - Train')\n",
        "    plt.plot(history.history['val_accuracy'], label=f'{model_name} - Val')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Comparison of Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "for model_name, history in histories.items():\n",
        "    plt.plot(history.history['loss'], label=f'{model_name} - Train')\n",
        "    plt.plot(history.history['val_loss'], label=f'{model_name} - Val')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Comparison of Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L0Yx1ZmcIdI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SBvWxPHfQBym67nXXyuIebv3lGFD7xZS",
      "authorship_tag": "ABX9TyNtyB8T0Aljru86etFYIRgc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}